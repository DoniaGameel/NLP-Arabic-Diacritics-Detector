# -*- coding: utf-8 -*-
"""LSTM_W2V.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CZL2e-_bwPDd78RD7rC2R1LUa6IMAWad
"""

import pickle
import numpy as np

with open('drive/MyDrive/X.pickle', 'rb') as file:
    X = pickle.load(file)

with open('drive/MyDrive/Y.pickle', 'rb') as file:
    Y = pickle.load(file)

x_train = np.array(X)
y_train = np.array(Y)

from google.colab import drive
drive.mount('/content/drive')

print(x_train.shape)
print(y_train.shape)
x_train = x_train.reshape((x_train.shape[0], 1, -1))

from tensorflow import keras
from tensorflow.keras import regularizers
from tensorflow.keras.layers import Bidirectional, Dropout, Activation, Dense, LSTM
from tensorflow.compat.v1.keras.layers import CuDNNLSTM
from tensorflow.keras.models import Sequential

# fraction of the input to drop; helps prevent overfitting
seq_len = 25
dropout = 0.5
window_size = seq_len // 2

# Adjust input shape
input_shape = (1, 86)
num_classes =  15

model = Sequential()

model.add(LSTM(window_size, return_sequences=True, input_shape=input_shape))
model.add(Dropout(rate=dropout))
model.add(Bidirectional(LSTM((window_size * 2), return_sequences=True)))
model.add(Dropout(rate=dropout))
model.add(Bidirectional(LSTM(window_size, return_sequences=False)))
model.add(Dense(units=num_classes, activation='softmax', kernel_regularizer=regularizers.l2(0.01)))

import pickle
import tensorflow as tf
from tensorflow.keras.optimizers import Adam

batch_size = 64
y_train = tf.cast(y_train, tf.int32)
y_train = tf.one_hot(y_train, num_classes)
optimizer = Adam(learning_rate=0.001)

model.compile(loss='categorical_crossentropy', optimizer=optimizer , metrics=['accuracy'])

total = 0
step = 500
for r in range(0, 57500, step):
  print(total, r)
  with open('drive/MyDrive/word2vec_embedding/X_word2vec_'+str(r)+'_'+str((r+step))+'.pickle', 'rb') as file:
      X = pickle.load(file)
  l = len(X)
  x_train = np.array(X)
  x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))

  history = model.fit(
    x_train,
    y_train[total:total+l],
    epochs=10,
    batch_size=batch_size,
    shuffle=False,
    validation_split=0.1
  )

  total += l

# with open('drive/MyDrive/Word2Vec_Model.pickle', 'wb') as file:
#   pickle.dump(model, file)
print(y_train[0:0+10])

import tensorflow as tf
from tensorflow.keras.optimizers import Adam

batch_size = 256
y_train = tf.cast(y_train, tf.int32)
y_train = tf.one_hot(y_train, num_classes)
optimizer = Adam(learning_rate=0.001)

model.compile(loss='categorical_crossentropy', optimizer=optimizer , metrics=['accuracy'])

history = model.fit(
    x_train,
    y_train,
    epochs=10,
    batch_size=batch_size,
    shuffle=False,
    validation_split=0.1
)

x_test = [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
       0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
       0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       1., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
       0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       1., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0.], [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0.]]

# x_test = np.array(x_test)
# x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))
# with open('drive/MyDrive/Word2Vec_Model.pickle', 'rb') as file:
#     model = pickle.load(file)
y_pred = model.predict(x_train[200:300])
# print(y_pred)
processed_output_list = [np.argmax(output) for output in y_pred]
print(processed_output_list)
print([np.argmax(output) for output in y_train[200:300]])

print(y_pred)
y_pred_ = [round(pred[0]) for pred in y_pred]
print(y_pred_)

len([3, 14, 3, 14, 14, 3, 14, 5, 8, 2, 14, 2, 9, 14, 3, 5, 14])

with open('linear.pickle', 'wb') as file:
    pickle.dump(model, file)