{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pickle\n",
    "#from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diacritics_dict = {'َ': 0, 'ً': 1, 'ُ': 2, 'ٌ': 3, 'ِ': 4, 'ٍ': 5, 'ْ': 6, 'ّ': 7, 'َّ': 8, 'ًّ': 9, 'ُّ': 10, 'ٌّ': 11, 'ِّ': 12, 'ٍّ': 13, '': 14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of allowed characters (Arabic characters with diacritics)\n",
    "allowed_characters = [\n",
    "     'ذ', 'ت', 'ع', 'س', 'خ', 'ئ', 'ط', 'ث', 'ص', 'ح', 'إ', 'غ', 'ا', 'ب', 'ج', 'ز', 'آ', 'ر',\n",
    "     'ش', 'ي', 'ض', 'ه', 'ق', 'ء', 'ن', 'د', 'ة', 'ف', 'ؤ', 'ظ', 'ل', 'م', 'ك', 'ى', 'أ', 'و', \n",
    "     'ّ', 'َ', 'ً', 'ٍ', 'ْ', 'ِ', 'ُ', 'ٌ'\n",
    " ]\n",
    "def clean_text(text):\n",
    "    \n",
    "    # Remove characters not in the allowed list\n",
    "    cleaned_text = ''.join(char if char in allowed_characters else ' ' for char in text)\n",
    "    \n",
    "    # Collapse consecutive spaces into a single space\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(text)\n",
    "\n",
    "    # Tokenize the cleaned text\n",
    "    tokens = tokenize_text(cleaned_text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract letters in X and corresponding Diacritics in Y and same for words\n",
    "def extract_arabic_and_Diacritics(arabic_tokens):\n",
    "    # List of characters without Diacritics\n",
    "    characters = []\n",
    "    # List of words without Diacritics\n",
    "    words = []\n",
    "    # List of Diacritics corresponding to each character in characters list\n",
    "    Diacritics_characters = []\n",
    "    # List of Diacritics corresponding to each word in words list\n",
    "    Diacritics_words = []\n",
    "\n",
    "    # All Diacritics letters in Arabic\n",
    "    Diacritics_set = ('َ', 'ُ', 'ِ', 'ّ', 'ْ', 'ٌ', 'ً', 'ٍ')\n",
    "\n",
    "    # For each word with Diacritics\n",
    "    for token in arabic_tokens:\n",
    "        token_diacritic = []\n",
    "        # Will be concatinated with letters without Diacritics\n",
    "        word_without_Diacritics = ''\n",
    "        # Check if the last letter in the word is Diacritics ==> Add it to Diacritics_words\n",
    "        # Look at 2 letters as the diacritic may be 2 letters\n",
    "        if token[-1] in Diacritics_set:\n",
    "            token_diacritic.append(token[-1])\n",
    "        # If the last character isn't Diacritics ==> Add None to Diacritics_words\n",
    "        else:\n",
    "            token_diacritic.append(None)\n",
    "        if(len(token)>=2):\n",
    "            if token[-2] in Diacritics_set:\n",
    "                token_diacritic.append(token[-2])\n",
    "            # If the last character isn't Diacritics ==> Add None to Diacritics_words\n",
    "            else:\n",
    "                token_diacritic.append(None)\n",
    "        Diacritics_words.append(token_diacritic)\n",
    "        # Loop over all characters of this word\n",
    "        for char in token:\n",
    "            # Check it isn't Diacritics (It is Arabic characher)\n",
    "            if char not in Diacritics_set:\n",
    "                # Add the character without Diacritics to characters list\n",
    "                characters.append(char)\n",
    "                # Concatinate the character without Diacritics to the word\n",
    "                word_without_Diacritics += char\n",
    "\n",
    "                # Extract Diacritics (it's the character after the Arabic letter)\n",
    "                Diacritics_index = token.index(char) + 1\n",
    "                # A list because it can be more than one Diacritics characters\n",
    "                Diacritics = []\n",
    "                # Check it is a Diacritics character\n",
    "                if Diacritics_index < len(token) and token[Diacritics_index] in Diacritics_set:\n",
    "                    Diacritics.append(token[Diacritics_index])\n",
    "                    Diacritics_index +=1\n",
    "                    # Diacritics may be 2 characters if it is (شدة + فتحة/ضمة/كسرة)\n",
    "                    # Check for the character after the Diacritics character\n",
    "                    if Diacritics_index < len(token) and token[Diacritics_index] in Diacritics_set:\n",
    "                        Diacritics.append(token[Diacritics_index])\n",
    "                    else:\n",
    "                        # The second Diacritics character is None\n",
    "                        Diacritics.append(None)\n",
    "                else:\n",
    "                    # No Diacritics\n",
    "                    Diacritics.extend([None, None])\n",
    "                # Add the Diacritics of the character to the list item corresponding to the character in characters list\n",
    "                Diacritics_characters.append(Diacritics)\n",
    "        # Add the word whithout Diacritics to the words list\n",
    "        words.append(word_without_Diacritics)\n",
    "        \n",
    "                \n",
    "    return characters, Diacritics_characters, words, Diacritics_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(dataset_path):\n",
    "    #dataset_path = r'/home/donia/Desktop/college/NLP/project/NLP-Arabic-Diacritics-Detector/dataset/train.txt'\n",
    "    # Get the path to the current script's directory\n",
    "    current_file_path = os.getcwd()\n",
    "    \n",
    "    # Construct the full path to the image\n",
    "    dataset_path = os.path.join(current_file_path, dataset_path)\n",
    "    \n",
    "    with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "        lines = [line.strip() for line in file.readlines()]\n",
    "    \n",
    "    # Split each line on dots to minimize length of line\n",
    "    split_lines = [line.split('.') for line in lines]\n",
    "    # Flatten the lines after splitting so each splitted part be a separate line\n",
    "    # Remove empty lines\n",
    "    flat_split_lines = [part for line in split_lines for part in line if part.strip()]\n",
    "    \n",
    "    #Lists to contain all the training data and its labels\n",
    "    X_letters_list =[]\n",
    "    Y_letters_list =[]\n",
    "    X_words_list =[]\n",
    "    Y_words_list =[]\n",
    "    \n",
    "    \n",
    "    tokenized_lines =[]\n",
    "    \n",
    "    \n",
    "    # Test the functions on the first 10 lines\n",
    "    for line in flat_split_lines:\n",
    "        cleaned_line = clean_text(line)\n",
    "        removed = set(line) - set(cleaned_line)\n",
    "        tokenized_line = tokenize_text(cleaned_line)\n",
    "        \n",
    "        X_letters, Y_letters, X_words, Y_words = extract_arabic_and_Diacritics(tokenized_line)\n",
    "        X_letters_list.append(X_letters)\n",
    "        Y_letters_list.append(Y_letters)\n",
    "        X_words_list.append(X_words)\n",
    "        Y_words_list.append(Y_words)\n",
    "        tokenized_lines.append(tokenized_line)\n",
    "\n",
    "    return X_letters_list, Y_letters_list, X_words_list, Y_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_letters_classes(Y_letters_list):\n",
    "    letters_diacritics_classes = []\n",
    "    for line in Y_letters_list:\n",
    "        line_letters_diacritics_classes=[]\n",
    "        for diacritic in line:\n",
    "            diacritic_string = \"\"\n",
    "            if(diacritic[0]):\n",
    "                diacritic_string = diacritic[0]\n",
    "            if(diacritic[1]):\n",
    "                diacritic_string+=diacritic[1]\n",
    "            line_letters_diacritics_classes.append(diacritics_dict[diacritic_string])\n",
    "        letters_diacritics_classes.append(line_letters_diacritics_classes)\n",
    "    return letters_diacritics_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing training dataset\n",
    "X_letters_list, Y_letters_list, X_words_list, Y_words_list = read_dataset(\"dataset/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_diacritics_classes = extract_letters_classes(Y_letters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing validation dataset\n",
    "val_X_letters_list, val_Y_letters_list, val_X_words_list, val_Y_words_list = read_dataset(\"dataset/val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_letters_diacritics_classes = extract_letters_classes(val_Y_letters_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_id_mapping(characters):\n",
    "    char_to_id = {char: idx for idx, char in enumerate(characters)}\n",
    "    id_to_char = {idx: char for idx, char in enumerate(characters)}\n",
    "    return char_to_id, id_to_char\n",
    "\n",
    "def generate_char_one_hot_vector(char_id, char_vector_size=45):\n",
    "    char_vector = np.zeros(char_vector_size)\n",
    "    char_vector[char_id] = 1\n",
    "    return char_vector\n",
    "\n",
    "def one_hot_encode_chars(char, char_to_id, char_vector_size=45):\n",
    "    char_id = char_to_id.get(char, None)\n",
    "\n",
    "    if char_id is not None:\n",
    "        return generate_char_one_hot_vector(char_id, char_vector_size)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = set([char for sequence in X_letters_list for char in sequence])\n",
    "\n",
    "char_to_id, _ = generate_id_mapping(list(characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate one hot encoding for train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [char for sequence in X_letters_list for char in sequence]\n",
    "\n",
    "X = [generate_char_one_hot_vector(char_to_id[char]) for char in characters ]\n",
    "\n",
    "Y = [item for sublist in letters_diacritics_classes for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate one hot encoding for validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [char for sequence in val_X_letters_list for char in sequence]\n",
    "\n",
    "val_X = [generate_char_one_hot_vector(char_to_id[char]) for char in characters ]\n",
    "\n",
    "val_Y = [item for sublist in val_letters_diacritics_classes for item in sublist]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
