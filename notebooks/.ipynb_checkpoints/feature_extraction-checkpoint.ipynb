{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_id_mapping(characters):\n",
    "    char_to_id = {char: idx for idx, char in enumerate(characters)}\n",
    "    id_to_char = {idx: char for idx, char in enumerate(characters)}\n",
    "    return char_to_id, id_to_char\n",
    "\n",
    "def generate_char_one_hot_vector(char_id, char_vector_size=45):\n",
    "    char_vector = np.zeros(char_vector_size)\n",
    "    char_vector[char_id] = 1\n",
    "    return char_vector\n",
    "\n",
    "def one_hot_encode_chars(char, char_to_id, char_vector_size=45):\n",
    "    char_id = char_to_id.get(char, None)\n",
    "\n",
    "    if char_id is not None:\n",
    "        return generate_char_one_hot_vector(char_id, char_vector_size)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_word_embeddings(words, word2vec_model):\n",
    "    word_embeddings = []\n",
    "    for word in words:\n",
    "        # Embeddings for individual words in the list\n",
    "        embeddings = []\n",
    "        for subword in word:\n",
    "            if subword in word2vec_model.wv:\n",
    "                embeddings.append(word2vec_model.wv[subword])\n",
    "            else:\n",
    "                # Handle out-of-vocabulary words\n",
    "                embeddings.append(np.zeros(word2vec_model.vector_size))\n",
    "        word_embeddings.append(embeddings)\n",
    "    return word_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_characters = [\n",
    "    'ء', 'إ', 'أ', 'ا', 'آ','ﺁ', 'ب', 'پ', 'ت', 'ث', 'ج', 'چ', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط',\n",
    "    'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'ھ',\n",
    "    'ؤ', 'و', 'ي', 'ئ', 'يء','ى', 'ة', 'ﻵ', 'ﻻ', 'ﻹ', 'ﻷ',\n",
    "    'َ', 'ُ', 'ِ', 'ّ', 'ْ', 'ٌ', 'ً', 'ٍ'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # Remove characters not in the allowed list\n",
    "    cleaned_text = ''.join(char if char in allowed_characters else ' ' for char in text)\n",
    "    \n",
    "    # Collapse consecutive spaces into a single space\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract letters in X and corresponding Diacritics in Y and same for words\n",
    "def extract_arabic_and_Diacritics(arabic_tokens):\n",
    "    # List of characters without Diacritics\n",
    "    characters = []\n",
    "    # List of words without Diacritics\n",
    "    words = []\n",
    "    # List of Diacritics corresponding to each character in characters list\n",
    "    Diacritics_characters = []\n",
    "    # List of Diacritics corresponding to each word in words list\n",
    "    Diacritics_words = []\n",
    "\n",
    "    # All Diacritics letters in Arabic\n",
    "    Diacritics_set = ('َ', 'ُ', 'ِ', 'ّ', 'ْ', 'ٌ', 'ً', 'ٍ')\n",
    "\n",
    "    # For each word with Diacritics\n",
    "    for token in arabic_tokens:\n",
    "        # Will be concatinated with letters without Diacritics\n",
    "        word_without_Diacritics = ''\n",
    "        # Check if the last letter in the word is Diacritics ==> Add it to Diacritics_words\n",
    "        if token[-1] in Diacritics_set:\n",
    "            Diacritics_words.append(token[-1])\n",
    "        # If the last character isn't Diacritics ==> Add None to Diacritics_words\n",
    "        else:\n",
    "            Diacritics_words.append(None)\n",
    "\n",
    "        # Loop over all characters of this word\n",
    "        for idx, char in enumerate(token):\n",
    "            # Check it isn't Diacritics (It is Arabic characher)\n",
    "            if char not in Diacritics_set:\n",
    "                # Add the character without Diacritics to characters list\n",
    "                characters.append(char)\n",
    "                # Concatinate the character without Diacritics to the word\n",
    "                word_without_Diacritics += char\n",
    "\n",
    "                # Extract Diacritics (it's the character after the Arabic letter)\n",
    "                Diacritics_index = idx + 1\n",
    "                # A list because it can be more than one Diacritics characters\n",
    "                Diacritics = []\n",
    "                # Check it is a Diacritics character\n",
    "                if Diacritics_index < len(token) and token[Diacritics_index] in Diacritics_set:\n",
    "                    Diacritics.append(token[Diacritics_index])\n",
    "                    Diacritics_index +=1\n",
    "                    # Diacritics may be 2 characters if it is (شدة + فتحة/ضمة/كسرة)\n",
    "                    # Check for the character after the Diacritics character\n",
    "                    if Diacritics_index < len(token) and token[Diacritics_index] in Diacritics_set:\n",
    "                        Diacritics.append(token[Diacritics_index])\n",
    "                    else:\n",
    "                        # The second Diacritics character is None\n",
    "                        Diacritics.append(None)\n",
    "                else:\n",
    "                    # No Diacritics\n",
    "                    Diacritics.extend([None, None])\n",
    "                # Add the Diacritics of the character to the list item corresponding to the character in characters list\n",
    "                Diacritics_characters.append(Diacritics)\n",
    "        # Add the word whithout Diacritics to the words list\n",
    "        words.append(word_without_Diacritics)\n",
    "        \n",
    "                \n",
    "    return characters, Diacritics_characters, words, Diacritics_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your dataset\n",
    "dataset_path = \"dataset/train.txt\"\n",
    "\n",
    "# Read and preprocess the data\n",
    "with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "    lines = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Split each line on dots to minimize the length of the line\n",
    "split_lines = [line.split('.') for line in lines]\n",
    "# Flatten the lines after splitting so each splitted part is a separate line\n",
    "# Remove empty lines\n",
    "flat_split_lines = [part for line in split_lines for part in line if part.strip()]\n",
    "\n",
    "# Lists to contain all the training data and its labels\n",
    "X_letters_list = []\n",
    "Y_letters_list = []\n",
    "X_words_list = []\n",
    "Y_words_list = []\n",
    "\n",
    "tokenized_lines = []\n",
    "\n",
    "# Test the functions on the first 10 lines\n",
    "for line in flat_split_lines:\n",
    "    cleaned_line = clean_text(line)\n",
    "    removed = set(line) - set(cleaned_line)\n",
    "    tokenized_line = tokenize_text(cleaned_line)\n",
    "\n",
    "    X_letters, Y_letters, X_words, Y_words = extract_arabic_and_Diacritics(tokenized_line)\n",
    "    X_letters_list.append(X_letters)\n",
    "    Y_letters_list.append(Y_letters)\n",
    "    X_words_list.append(X_words)\n",
    "    Y_words_list.append(Y_words)\n",
    "    tokenized_lines.append(tokenized_line)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي']\n",
      "[['َ', None], ['ْ', None], ['ُ', None], ['ُ', None], ['َ', None], ['ْ', None], ['َ', None], ['َ', None], ['َ', None], [None, None], ['ْ', None], ['َ', None], ['ّ', 'َ'], ['ُ', None], ['َ', None], ['َ', None], ['ُ', None], [None, None], ['َ', None], ['ْ', None], ['َ', None], [None, None], ['َ', None], [None, None], [None, None], ['ّ', 'َ'], ['ْ', None], ['َ', None], ['ِ', None], ['ّ', 'ُ']]\n",
      "['قَوْلُهُ', 'أَوْ', 'قَطَعَ', 'الْأَوَّلُ', 'يَدَهُ', 'إلَخْ', 'قَالَ', 'الزَّرْكَشِيُّ']\n"
     ]
    }
   ],
   "source": [
    "print(X_words_list[0])\n",
    "print(Y_letters_list[0])\n",
    "print(tokenized_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=X_words_list, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Extract features using Word2Vec\n",
    "word_embeddings = get_word_embeddings(X_words_list, word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = set([char for sequence in X_letters_list for char in sequence])\n",
    "\n",
    "char_to_id, _ = generate_id_mapping(list(characters))\n",
    "\n",
    "# Testing the function on the X_letters_list\n",
    "X_letters_one_hot = { char: one_hot_encode_chars(char, char_to_id, len(characters)) for char in characters }\n",
    "\n",
    "# Display the results for the first few characters\n",
    "# for char, one_hot_encoding in X_letters_one_hot.items():\n",
    "#     print(f\"Character: {char}, One-Hot Encoding: {one_hot_encoding} - Length: {len(one_hot_encoding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ِ', 'ٌ', 'ً', 'ٍ', 'ُ', 'ْ', 'ّ', 'َ'}\n",
      "{'َ': 0, 'ً': 1, 'ُ': 2, 'ٌ': 3, 'ِ': 4, 'ٍ': 5, 'ْ': 6, 'ّ': 7, 'َّ': 8, 'ًّ': 9, 'ُّ': 10, 'ٌّ': 11, 'ِّ': 12, 'ٍّ': 13, '': 14}\n",
      "{'ط', 'د', 'ك', 'ظ', 'ش', 'أ', 'خ', 'ض', 'ذ', 'ة', 'ص', 'ت', 'م', 'ه', 'ح', 'ج', 'ع', 'ن', 'آ', 'ر', 'غ', 'ئ', 'ز', 'ي', 'إ', 'ء', 'ى', 'ؤ', 'و', 'ق', 'ب', 'ث', 'س', 'ف', 'ل', 'ا'}\n",
      "8\n",
      "15\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('diacritics.pickle', 'rb') as file:\n",
    "    diacritics = pickle.load(file)\n",
    "    \n",
    "with open('diacritic2id.pickle', 'rb') as file:\n",
    "    diacritic2id = pickle.load(file)\n",
    "    \n",
    "with open('arabic_letters.pickle', 'rb') as file:\n",
    "    arabic_letters = pickle.load(file)\n",
    "    \n",
    "print(diacritics)\n",
    "print(diacritic2id)\n",
    "print(arabic_letters)\n",
    "\n",
    "print(len(diacritics))\n",
    "print(len(diacritic2id))\n",
    "print(len(arabic_letters))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Feature Vectors with Hot Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "joined_sentences = [\"\".join(sentence) for sentence in X_words_list]\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for sentence_list, diacritics_list in zip(joined_sentences, Y_letters_list):\n",
    "    length = len(sentence_list)\n",
    "    for idx in range(length):\n",
    "        X.append(X_letters_one_hot[sentence_list[idx]])\n",
    "        d = ''\n",
    "        if diacritics_list[idx][0] != None:\n",
    "            d += diacritics_list[idx][0]\n",
    "            if diacritics_list[idx][1] != None:\n",
    "                d += diacritics_list[idx][1]\n",
    "            \n",
    "        Y.append(diacritic2id[d])\n",
    "        \n",
    "with open('X.pickle', 'wb') as file:\n",
    "    pickle.dump(X, file)\n",
    "    \n",
    "with open('Y.pickle', 'wb') as file:\n",
    "    pickle.dump(Y, file)\n",
    "    \n",
    "print(X[3])\n",
    "print(Y[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Feature Vectors with Hot Vector + Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57778\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i, sentence in enumerate(X_words_list):\n",
    "    char_index = 0\n",
    "    diacritics_list = Y_letters_list[i]\n",
    "    for j, word in enumerate(sentence):\n",
    "        word_vector = word_embeddings[i][j]\n",
    "        for k, char in enumerate(word):\n",
    "            char_hot_vector = X_letters_one_hot[char]\n",
    "            X.append(list(char_hot_vector) + list(word_vector))\n",
    "            d = ''\n",
    "            if diacritics_list[char_index][0] != None:\n",
    "                d += diacritics_list[char_index][0]\n",
    "                if diacritics_list[char_index][1] != None:\n",
    "                    d += diacritics_list[char_index][1]\n",
    "            \n",
    "            Y.append(diacritic2id[d])\n",
    "            char_index += 1\n",
    "            \n",
    "print(X[6])\n",
    "print(Y[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.])]\n",
      "[0, 6, 2, 2, 0, 6, 0, 0, 0, 14]\n"
     ]
    }
   ],
   "source": [
    "with open('X_word2vec.pickle', 'rb') as file:\n",
    "    x = pickle.load(file)\n",
    "    \n",
    "# with open('Y.pickle', 'rb') as file:\n",
    "#     y = pickle.load(file)\n",
    "    \n",
    "# print(x[:10])\n",
    "# print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_diacritic_freq = {}\n",
    "\n",
    "for i, words in enumerate(X_words_list):\n",
    "    for j, word in enumerate(words):\n",
    "        diacritic = Y_words_list[i][j]\n",
    "        prev_freq = word_diacritic_freq.get((word, diacritic), 0)\n",
    "        word_diacritic_freq.__setitem__((word, diacritic), prev_freq + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('قوله', 'ُ'), 20797), (('أو', 'ْ'), 31767), (('قطع', 'َ'), 598), (('الأول', 'ُ'), 685), (('يده', 'ُ'), 332), (('إلخ', 'ْ'), 4808), (('قال', 'َ'), 16517), (('الزركشي', 'ُ'), 228), (('ابن', 'ُ'), 3789), (('عرفة', 'َ'), 495), (('بلفظ', 'ٍ'), 41), (('يقتضيه', None), 11), (('كإنكار', 'ِ'), 1), (('غير', 'ِ'), 3009), (('حديث', 'ٍ'), 38), (('بالإسلام', 'ِ'), 56), (('وجوب', 'َ'), 105), (('ما', None), 17401), (('علم', 'َ'), 1067), (('وجوبه', 'ُ'), 64), (('من', 'ْ'), 44648), (('الدين', 'ِ'), 973), (('ضرورة', 'ً'), 44), (('كإلقاء', 'ِ'), 4), (('مصحف', 'ٍ'), 25), (('بقذر', 'ٍ'), 2), (('وشد', 'ِ'), 24), (('زنار', 'ٍ'), 2), (('قول', 'ُ'), 1393), (('ابن', 'ِ'), 2338), (('شاس', 'ٍ'), 91), (('بفعل', 'ٍ'), 43), (('يتضمنه', 'ُ'), 5), (('هو', 'َ'), 5218), (('كلبس', 'ِ'), 8), (('الزنار', 'ِ'), 1), (('وإلقاء', 'ِ'), 2), (('المصحف', 'ِ'), 49), (('في', None), 46154), (('صريح', 'ِ'), 6), (('النجاسة', 'ِ'), 74), (('والسجود', 'ِ'), 12), (('للصنم', 'ِ'), 1), (('ونحو', 'ِ'), 105), (('ذلك', 'َ'), 12839), (('وسحر', 'ٍ'), 4), (('محمد', 'ٌ'), 309), (('مالك', 'ٍ'), 1039), (('وأصحابه', 'ِ'), 27), (('أن', 'َ'), 10542)]\n"
     ]
    }
   ],
   "source": [
    "print(list(word_diacritic_freq.items())[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_diacritic_freq = {}\n",
    "\n",
    "for i, chars in enumerate(X_letters_list):\n",
    "    for j, char in enumerate(chars):\n",
    "        diacritic = tuple(Y_letters_list[i][j])\n",
    "        prev_freq = char_diacritic_freq.get(char, {})\n",
    "        freq = prev_freq.get(diacritic, 0)\n",
    "        prev_freq.__setitem__(diacritic, freq + 1)\n",
    "        char_diacritic_freq.__setitem__(char, prev_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ق', {('َ', None): 148128, ('ْ', None): 26576, ('ِ', None): 32778, ('ُ', None): 23409, ('ّ', 'َ'): 3368, ('ّ', 'ِ'): 2848, ('ّ', 'ٍ'): 371, ('ٌ', None): 2184, ('ّ', 'ُ'): 2235, ('ٍ', None): 1860, ('ً', None): 2315, ('ّ', 'ً'): 232, ('ّ', None): 50, ('ّ', 'ٌ'): 378, (None, None): 256})\n"
     ]
    }
   ],
   "source": [
    "print(list(char_diacritic_freq.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('َ', None): 0.3511366491057032, ('ْ', None): 0.1471525159977671, ('ِ', None): 0.15130519412252538, ('ُ', None): 0.09701240906100693, ('ّ', 'َ'): 0.035844792981553686, ('ّ', 'ِ'): 0.0076095512674523, ('ّ', 'ٍ'): 0.0006978405499002691, ('ٌ', None): 0.006951583899281061, ('ّ', 'ُ'): 0.005184591278334206, ('ٍ', None): 0.010460783109289159, ('ً', None): 0.006993013691708222, ('ّ', 'ً'): 0.00041381896713372173, ('ّ', None): 0.0007520824457658871, ('ّ', 'ٌ'): 0.0005680431655330949, (None, None): 0.1779171303570458}\n"
     ]
    }
   ],
   "source": [
    "diacritics_probability = {}\n",
    "\n",
    "total = 0\n",
    "\n",
    "for char, freq in char_diacritic_freq.items():\n",
    "    for diacritic, f in freq.items():\n",
    "        updated_freq = diacritics_probability.get(diacritic, 0) + f\n",
    "        total += f\n",
    "        diacritics_probability.__setitem__(diacritic, updated_freq)\n",
    "        \n",
    "for diacritic, f in diacritics_probability.items():\n",
    "    diacritics_probability[diacritic] /= total\n",
    "    \n",
    "print(diacritics_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_character(char):\n",
    "    max_prob = 0\n",
    "    best_diacritic = None\n",
    "    for diacritic in diacritics_probability.keys():\n",
    "        count = char_diacritic_freq[char].get(diacritic, 0)\n",
    "        total_count = diacritics_probability[diacritic] * total\n",
    "        prob = diacritics_probability[diacritic] * count / total_count\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            best_diacritic = diacritic\n",
    "    return best_diacritic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "يدَهُ\n"
     ]
    }
   ],
   "source": [
    "text = 'يده'\n",
    "output = ''\n",
    "\n",
    "for idx, char in enumerate(text):\n",
    "    if char == ' ':\n",
    "        output += ' '\n",
    "        continue\n",
    "    best_diacritic = classify_character(char)\n",
    "    output += char\n",
    "    if best_diacritic[0]:\n",
    "        output += best_diacritic[0]\n",
    "        if best_diacritic[1]:\n",
    "            output += best_diacritic[1]\n",
    "            \n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
