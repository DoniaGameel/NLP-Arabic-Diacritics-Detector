{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI7rKvBP6ClH"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y6YhY0vF6ClO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "#from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "3EEVzHQF8l1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5a414fc-712e-4b44-cad9-4a2ee5c8987e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xv5IH1dd6ClT"
      },
      "outputs": [],
      "source": [
        "diacritics_dict = {'َ': 0, 'ً': 1, 'ُ': 2, 'ٌ': 3, 'ِ': 4, 'ٍ': 5, 'ْ': 6, 'ّ': 7, 'َّ': 8, 'ًّ': 9, 'ُّ': 10, 'ٌّ': 11, 'ِّ': 12, 'ٍّ': 13, '': 14}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "S2z6lk0D6ClW"
      },
      "outputs": [],
      "source": [
        "# Define a list of allowed characters (Arabic characters with diacritics)\n",
        "allowed_characters = [\n",
        "     'ذ', 'ت', 'ع', 'س', 'خ', 'ئ', 'ط', 'ث', 'ص', 'ح', 'إ', 'غ', 'ا', 'ب', 'ج', 'ز', 'آ', 'ر',\n",
        "     'ش', 'ي', 'ض', 'ه', 'ق', 'ء', 'ن', 'د', 'ة', 'ف', 'ؤ', 'ظ', 'ل', 'م', 'ك', 'ى', 'أ', 'و',\n",
        "     'ّ', 'َ', 'ً', 'ٍ', 'ْ', 'ِ', 'ُ', 'ٌ'\n",
        " ]\n",
        "def clean_text(text):\n",
        "\n",
        "    # Remove characters not in the allowed list\n",
        "    cleaned_text = ''.join(char if char in allowed_characters else ' ' for char in text)\n",
        "\n",
        "    # Collapse consecutive spaces into a single space\n",
        "    cleaned_text = ' '.join(cleaned_text.split())\n",
        "\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zo9NiyI66ClY"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(text):\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uK7yOR9-6ClZ"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    # Clean the text\n",
        "    cleaned_text = clean_text(text)\n",
        "\n",
        "    # Tokenize the cleaned text\n",
        "    tokens = tokenize_text(cleaned_text)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PLNjTGtt6Cla"
      },
      "outputs": [],
      "source": [
        "# Extract letters in X and corresponding Diacritics in Y and same for words\n",
        "def extract_arabic_and_Diacritics(arabic_tokens):\n",
        "    # List of characters without Diacritics\n",
        "    characters = []\n",
        "    # List of words without Diacritics\n",
        "    words = []\n",
        "    # List of Diacritics corresponding to each character in characters list\n",
        "    Diacritics_characters = []\n",
        "    # List of Diacritics corresponding to each word in words list\n",
        "    Diacritics_words = []\n",
        "\n",
        "    # All Diacritics letters in Arabic\n",
        "    Diacritics_set = ('َ', 'ُ', 'ِ', 'ّ', 'ْ', 'ٌ', 'ً', 'ٍ')\n",
        "\n",
        "    # For each word with Diacritics\n",
        "    for token in arabic_tokens:\n",
        "        token_diacritic = []\n",
        "        # Will be concatinated with letters without Diacritics\n",
        "        word_without_Diacritics = ''\n",
        "        # Check if the last letter in the word is Diacritics ==> Add it to Diacritics_words\n",
        "        # Look at 2 letters as the diacritic may be 2 letters\n",
        "        if token[-1] in Diacritics_set:\n",
        "            token_diacritic.append(token[-1])\n",
        "        # If the last character isn't Diacritics ==> Add None to Diacritics_words\n",
        "        else:\n",
        "            token_diacritic.append(None)\n",
        "        if(len(token)>=2):\n",
        "            if token[-2] in Diacritics_set:\n",
        "                token_diacritic.append(token[-2])\n",
        "            # If the last character isn't Diacritics ==> Add None to Diacritics_words\n",
        "            else:\n",
        "                token_diacritic.append(None)\n",
        "        Diacritics_words.append(token_diacritic)\n",
        "        # Loop over all characters of this word\n",
        "        for char in token:\n",
        "            # Check it isn't Diacritics (It is Arabic characher)\n",
        "            if char not in Diacritics_set:\n",
        "                # Add the character without Diacritics to characters list\n",
        "                characters.append(char)\n",
        "                # Concatinate the character without Diacritics to the word\n",
        "                word_without_Diacritics += char\n",
        "\n",
        "                # Extract Diacritics (it's the character after the Arabic letter)\n",
        "                Diacritics_index = token.index(char) + 1\n",
        "                # A list because it can be more than one Diacritics characters\n",
        "                Diacritics = []\n",
        "                # Check it is a Diacritics character\n",
        "                if Diacritics_index < len(token) and token[Diacritics_index] in Diacritics_set:\n",
        "                    Diacritics.append(token[Diacritics_index])\n",
        "                    Diacritics_index +=1\n",
        "                    # Diacritics may be 2 characters if it is (شدة + فتحة/ضمة/كسرة)\n",
        "                    # Check for the character after the Diacritics character\n",
        "                    if Diacritics_index < len(token) and token[Diacritics_index] in Diacritics_set:\n",
        "                        Diacritics.append(token[Diacritics_index])\n",
        "                    else:\n",
        "                        # The second Diacritics character is None\n",
        "                        Diacritics.append(None)\n",
        "                else:\n",
        "                    # No Diacritics\n",
        "                    Diacritics.extend([None, None])\n",
        "                # Add the Diacritics of the character to the list item corresponding to the character in characters list\n",
        "                Diacritics_characters.append(Diacritics)\n",
        "        # Add the word whithout Diacritics to the words list\n",
        "        words.append(word_without_Diacritics)\n",
        "\n",
        "\n",
        "    return characters, Diacritics_characters, words, Diacritics_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mzdRsp8b6Cld"
      },
      "outputs": [],
      "source": [
        "def read_dataset(dataset_path):\n",
        "    #dataset_path = r'/home/donia/Desktop/college/NLP/project/NLP-Arabic-Diacritics-Detector/dataset/train.txt'\n",
        "    # Get the path to the current script's directory\n",
        "    current_file_path = os.getcwd()\n",
        "\n",
        "    # Construct the full path to the image\n",
        "    dataset_path = os.path.join(current_file_path, dataset_path)\n",
        "\n",
        "    with open(dataset_path, 'r', encoding='utf-8') as file:\n",
        "        lines = [line.strip() for line in file.readlines()]\n",
        "\n",
        "    # Split each line on dots to minimize length of line\n",
        "    split_lines = [line.split('.') for line in lines]\n",
        "    # Flatten the lines after splitting so each splitted part be a separate line\n",
        "    # Remove empty lines\n",
        "    flat_split_lines = [part for line in split_lines for part in line if part.strip()]\n",
        "\n",
        "    #Lists to contain all the training data and its labels\n",
        "    X_letters_list =[]\n",
        "    Y_letters_list =[]\n",
        "    X_words_list =[]\n",
        "    Y_words_list =[]\n",
        "\n",
        "\n",
        "    tokenized_lines =[]\n",
        "\n",
        "\n",
        "    # Test the functions on the first 10 lines\n",
        "    for line in flat_split_lines:\n",
        "        cleaned_line = clean_text(line)\n",
        "        removed = set(line) - set(cleaned_line)\n",
        "        tokenized_line = tokenize_text(cleaned_line)\n",
        "\n",
        "        X_letters, Y_letters, X_words, Y_words = extract_arabic_and_Diacritics(tokenized_line)\n",
        "        X_letters_list.append(X_letters)\n",
        "        Y_letters_list.append(Y_letters)\n",
        "        X_words_list.append(X_words)\n",
        "        Y_words_list.append(Y_words)\n",
        "        tokenized_lines.append(tokenized_line)\n",
        "\n",
        "    return X_letters_list, Y_letters_list, X_words_list, Y_words_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iRTe3thW6Cle"
      },
      "outputs": [],
      "source": [
        "def extract_letters_classes(Y_letters_list):\n",
        "    letters_diacritics_classes = []\n",
        "    for line in Y_letters_list:\n",
        "        line_letters_diacritics_classes=[]\n",
        "        for diacritic in line:\n",
        "            diacritic_string = \"\"\n",
        "            if(diacritic[0]):\n",
        "                diacritic_string = diacritic[0]\n",
        "            if(diacritic[1]):\n",
        "                diacritic_string+=diacritic[1]\n",
        "            line_letters_diacritics_classes.append(diacritics_dict[diacritic_string])\n",
        "        letters_diacritics_classes.append(line_letters_diacritics_classes)\n",
        "    return letters_diacritics_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rjUP3a816Clf"
      },
      "outputs": [],
      "source": [
        "# preprocessing training dataset\n",
        "X_letters_list, Y_letters_list, X_words_list, Y_words_list = read_dataset(\"train.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Em3irCQc6Clg"
      },
      "outputs": [],
      "source": [
        "letters_diacritics_classes = extract_letters_classes(Y_letters_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MscJ58pS6Clh"
      },
      "outputs": [],
      "source": [
        "# preprocessing validation dataset\n",
        "val_X_letters_list, val_Y_letters_list, val_X_words_list, val_Y_words_list = read_dataset(\"val.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0w-h2_rl6Clh"
      },
      "outputs": [],
      "source": [
        "val_letters_diacritics_classes = extract_letters_classes(val_Y_letters_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQqfUgvg6Cli"
      },
      "source": [
        "# One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ckvmn3Au6Cli"
      },
      "outputs": [],
      "source": [
        "def generate_id_mapping(characters):\n",
        "    char_to_id = {char: idx for idx, char in enumerate(characters)}\n",
        "    id_to_char = {idx: char for idx, char in enumerate(characters)}\n",
        "    return char_to_id, id_to_char\n",
        "\n",
        "def generate_char_one_hot_vector(char_id, char_vector_size=36):\n",
        "    char_vector = np.zeros(char_vector_size)\n",
        "    char_vector[char_id] = 1\n",
        "    return char_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdfoU3Su6Clj"
      },
      "source": [
        "Generate one hot encoding for train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jBx5Kcxj6Clj"
      },
      "outputs": [],
      "source": [
        "characters = [\n",
        "     'ذ', 'ت', 'ع', 'س', 'خ', 'ئ', 'ط', 'ث', 'ص', 'ح', 'إ', 'غ', 'ا', 'ب', 'ج', 'ز', 'آ', 'ر',\n",
        "     'ش', 'ي', 'ض', 'ه', 'ق', 'ء', 'ن', 'د', 'ة', 'ف', 'ؤ', 'ظ', 'ل', 'م', 'ك', 'ى', 'أ', 'و',\n",
        "\n",
        " ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_id, _ = generate_id_mapping(characters)"
      ],
      "metadata": {
        "id": "XQB0VAFg_07b"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_characters = [char for sequence in X_letters_list for char in sequence]\n",
        "\n",
        "X = [generate_char_one_hot_vector(char_to_id[char]) for char in train_characters ]"
      ],
      "metadata": {
        "id": "iFK8_OPZ9wYw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_letters_list\n",
        "del train_characters\n",
        "del Y_letters_list\n",
        "del X_words_list\n",
        "del Y_words_list\n",
        "del allowed_characters\n",
        "del diacritics_dict"
      ],
      "metadata": {
        "id": "KDX_6iV8DUru"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = [item for sublist in letters_diacritics_classes for item in sublist]"
      ],
      "metadata": {
        "id": "vLqx4G-qD9a0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_msaoug66Clr"
      },
      "source": [
        "Generate one hot encoding for validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RsjeyRwG6Clr"
      },
      "outputs": [],
      "source": [
        "characters = [char for sequence in val_X_letters_list for char in sequence]\n",
        "\n",
        "val_X = [generate_char_one_hot_vector(char_to_id[char]) for char in characters ]\n",
        "\n",
        "val_Y = [item for sublist in val_letters_diacritics_classes for item in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(X)\n",
        "del X"
      ],
      "metadata": {
        "id": "nt8qz7iFX-7n"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(Y)\n",
        "del Y"
      ],
      "metadata": {
        "id": "D9EwhC4MYH64"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.array(val_X)\n",
        "del val_X"
      ],
      "metadata": {
        "id": "O9m1UynNYIee"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.array(val_Y)\n",
        "del val_Y"
      ],
      "metadata": {
        "id": "W63TuLSqYIti"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test.shape)\n",
        "print(x_test.shape)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N5AYDI_cnt3",
        "outputId": "f1859111-a610-490c-d580-357b0fcb5feb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(421099,)\n",
            "(421099, 36)\n",
            "(8351478, 36)\n",
            "(8351478,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional LSTM\n"
      ],
      "metadata": {
        "id": "w_CyBR7_aQGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape input data for LSTM\n",
        "x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
        "x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))"
      ],
      "metadata": {
        "id": "dgfJTLmWaSzJ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Bidirectional, Dropout, Activation, Dense, LSTM, BatchNormalization\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "bT28RZMmbMDk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = 0.5  # Adjusted dropout rate\n",
        "window_size = 256\n",
        "input_shape = (1, 36)\n",
        "num_classes = 15\n",
        "batch_size = 256"
      ],
      "metadata": {
        "id": "BknyCZImbsyC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Bidirectional(LSTM(window_size,\n",
        "                             return_sequences=True,\n",
        "                             input_shape=input_shape,\n",
        "                             kernel_initializer=glorot_uniform())))  # Apply Glorot Uniform initializer\n",
        "model.add(Dropout(rate=dropout))\n",
        "\n",
        "model.add(Bidirectional(LSTM(window_size, return_sequences=True, kernel_initializer=glorot_uniform())))\n",
        "model.add(Dropout(rate=dropout))\n",
        "model.add(Activation('relu'))  # Add ReLU activation after LSTM\n",
        "\n",
        "model.add(Bidirectional(LSTM(window_size, return_sequences=False, kernel_initializer=glorot_uniform())))\n",
        "model.add(Dense(units=num_classes, activation='softmax'))"
      ],
      "metadata": {
        "id": "UCaY2VW4aeff"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "y_train = tf.cast(y_train, tf.int32)\n",
        "y_train = tf.one_hot(y_train, num_classes)\n",
        "\n",
        "y_test = tf.cast(y_test, tf.int32)\n",
        "y_test = tf.one_hot(y_test, num_classes)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer , metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    validation_data=(x_test, y_test)\n",
        ")"
      ],
      "metadata": {
        "id": "JKSd5Ch3qiXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c33e33e0-c8b6-4fe9-c58e-b7bd50b5367f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "32623/32623 [==============================] - 412s 12ms/step - loss: 1.2824 - accuracy: 0.5176 - val_loss: 1.2808 - val_accuracy: 0.5175\n",
            "Epoch 2/50\n",
            "32623/32623 [==============================] - 392s 12ms/step - loss: 1.2768 - accuracy: 0.5183 - val_loss: 1.2806 - val_accuracy: 0.5156\n",
            "Epoch 3/50\n",
            "32623/32623 [==============================] - 389s 12ms/step - loss: 1.2767 - accuracy: 0.5184 - val_loss: 1.2805 - val_accuracy: 0.5175\n",
            "Epoch 4/50\n",
            "32623/32623 [==============================] - 389s 12ms/step - loss: 1.2766 - accuracy: 0.5184 - val_loss: 1.2804 - val_accuracy: 0.5156\n",
            "Epoch 5/50\n",
            "32623/32623 [==============================] - 388s 12ms/step - loss: 1.2766 - accuracy: 0.5184 - val_loss: 1.2804 - val_accuracy: 0.5156\n",
            "Epoch 6/50\n",
            "32623/32623 [==============================] - 389s 12ms/step - loss: 1.2766 - accuracy: 0.5184 - val_loss: 1.2803 - val_accuracy: 0.5156\n",
            "Epoch 7/50\n",
            "32623/32623 [==============================] - 396s 12ms/step - loss: 1.2765 - accuracy: 0.5184 - val_loss: 1.2803 - val_accuracy: 0.5156\n",
            "Epoch 8/50\n",
            "32623/32623 [==============================] - 387s 12ms/step - loss: 1.2765 - accuracy: 0.5185 - val_loss: 1.2803 - val_accuracy: 0.5156\n",
            "Epoch 9/50\n",
            "32623/32623 [==============================] - 394s 12ms/step - loss: 1.2765 - accuracy: 0.5184 - val_loss: 1.2803 - val_accuracy: 0.5156\n",
            "Epoch 10/50\n",
            "32623/32623 [==============================] - 396s 12ms/step - loss: 1.2766 - accuracy: 0.5184 - val_loss: 1.2804 - val_accuracy: 0.5156\n",
            "Epoch 11/50\n",
            "32623/32623 [==============================] - 395s 12ms/step - loss: 1.2766 - accuracy: 0.5184 - val_loss: 1.2803 - val_accuracy: 0.5156\n",
            "Epoch 12/50\n",
            "32623/32623 [==============================] - 396s 12ms/step - loss: 1.2766 - accuracy: 0.5184 - val_loss: 1.2803 - val_accuracy: 0.5156\n",
            "Epoch 13/50\n",
            "32623/32623 [==============================] - 393s 12ms/step - loss: 1.2766 - accuracy: 0.5184 - val_loss: 1.2803 - val_accuracy: 0.5156\n",
            "Epoch 14/50\n",
            "32623/32623 [==============================] - 386s 12ms/step - loss: 1.2766 - accuracy: 0.5185 - val_loss: 1.2803 - val_accuracy: 0.5156\n",
            "Epoch 15/50\n",
            "32623/32623 [==============================] - 390s 12ms/step - loss: 1.2766 - accuracy: 0.5184 - val_loss: 1.2802 - val_accuracy: 0.5156\n",
            "Epoch 16/50\n",
            "32623/32623 [==============================] - 391s 12ms/step - loss: 1.2766 - accuracy: 0.5185 - val_loss: 1.2802 - val_accuracy: 0.5156\n",
            "Epoch 17/50\n",
            "32623/32623 [==============================] - 392s 12ms/step - loss: 1.2766 - accuracy: 0.5185 - val_loss: 1.2802 - val_accuracy: 0.5156\n",
            "Epoch 18/50\n",
            "32623/32623 [==============================] - 389s 12ms/step - loss: 1.2766 - accuracy: 0.5185 - val_loss: 1.2802 - val_accuracy: 0.5156\n",
            "Epoch 19/50\n",
            "32623/32623 [==============================] - 388s 12ms/step - loss: 1.2765 - accuracy: 0.5185 - val_loss: 1.2802 - val_accuracy: 0.5156\n",
            "Epoch 20/50\n",
            "20075/32623 [=================>............] - ETA: 2:26 - loss: 1.2773 - accuracy: 0.5181"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}